"""
AI ë¶„ì„ ì„œë¹„ìŠ¤
ETF_AI ëª¨ë“ˆê³¼ ì—°ë™í•˜ì—¬ íˆ¬ì ê²°ì •ì„ ë¶„ì„í•˜ê³  ì•Œë¦¼ ì—¬ë¶€ë¥¼ ê²°ì •
"""

import httpx
import logging
from typing import Dict, Optional
from datetime import datetime, timezone, timedelta
import json
import numpy as np
from config.timezone_config import get_kst_now

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from config.notification_config import NOTIFICATION_TYPES
from models import User, InvestmentSettings
from crud.notification import get_notifications_by_user_id_and_type
from crud.user import update_user_investment_settings # crud ì¶”ê°€

logger = logging.getLogger(__name__)

import os

# ETF_AI ì„œë¹„ìŠ¤ URL (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
AI_SERVICE_URL = os.getenv("ETF_AI_SERVICE_URL", "http://localhost:8001")
MAX_RETRIES = int(os.getenv("AI_SERVICE_MAX_RETRIES", "3"))
RETRY_DELAY = int(os.getenv("AI_SERVICE_RETRY_DELAY", "5"))

# ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
try:
    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    logger.info("âœ… Sentence Transformer ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    embedding_model = None
    logger.error(f"âŒ Sentence Transformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

def create_integrated_analysis_messages(
    user: User,
    user_setting: InvestmentSettings,
    etf_data_list: list,
) -> list:
    """
    ì‚¬ìš©ìì˜ ëª¨ë“  ETFë¥¼ í¬í•¨í•œ í†µí•© ë¶„ì„ ë©”ì‹œì§€ ìƒì„± (êµ¬ì¡°ì /êµ¬ì²´ì  í”„ë¡¬í”„íŠ¸)
    """
    try:
        # 1. ì‚¬ìš©ì ì •ë³´
        user_info = f"""[ì‚¬ìš©ì ì •ë³´]\n- ì´ë¦„: {user.name}\n- ìœ„í—˜ ì„±í–¥(0~10): {user_setting.risk_level}\n- íˆ¬ì ëª©í‘œ/í˜ë¥´ì†Œë‚˜: {user_setting.persona or 'ë¯¸ì…ë ¥'}"""
        
        # 2. ETF ì •ë³´
        etf_info = "[ë³´ìœ  ETF ëª©ë¡]\n" + "\n".join([
            f"- {etf_data['etf'].symbol}: {etf_data['etf_setting'].amount:,}ë§Œì›, ì£¼ê¸°: {etf_data['etf_setting'].cycle}, ì´ë¦„: {etf_data['etf'].name}"
            for etf_data in etf_data_list
        ])
        
        # 3. ìƒˆë¡œìš´ ì¶œë ¥ í¬ë§· ë° ê·œì¹™
        output_format_and_rules = (
            "[ì¶œë ¥ í¬ë§·]\n"
            "### ETF ë¶„ì„ ê²°ê³¼\n\n"
            "#### SPY (ë¯¸êµ­ S&P500)\n"
            "- **ê¶Œê³  ì‚¬í•­**: ë¹„ì¤‘ ìœ ì§€ (ì‹œì¥ ì•ˆì •, ì¶”ê°€ ë§¤ìˆ˜ ë¶ˆí•„ìš”)\n"
            "- **ì´ìœ **: ECBì˜ ì£¼ìš” ì •ì±…ê¸ˆë¦¬ ë™ê²°ë¡œ ì¸í•œ ê¸€ë¡œë²Œ ê¸ˆìœµì‹œì¥ì˜ ì•ˆì •ì„¸ê°€ ìœ ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n"
            "#### QQQ (ë¯¸êµ­ ë‚˜ìŠ¤ë‹¥)\n"
            "- **ê¶Œê³  ì‚¬í•­**: ë¹„ì¤‘ 10% ì¦ê°€ ê¶Œê³  (ê¸°ìˆ ì£¼ ê°•ì„¸, ì„±ì¥ ê¸°ëŒ€)\n"
            "- **ì´ìœ **: ê¸°ìˆ ì£¼ ì¤‘ì‹¬ì˜ ë‚˜ìŠ¤ë‹¥ ì‹œì¥ì€ ìµœê·¼ ê¸ì •ì ì¸ ê²½ì œ ì‹ í˜¸ë“¤ë¡œ ê°•ì„¸ë¥¼ ë³´ì…ë‹ˆë‹¤.\n\n"
            "### ì¢…í•© ì˜ê²¬:\n"
            "ì´ë²ˆ ì£¼ëŠ” ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ëœ ì‹œì¥ ëª¨ìŠµì„ ë³´ì˜€ìŠµë‹ˆë‹¤. í˜„ ìƒí™©ì—ì„œëŠ” ì ì§„ì ì´ê³  ì•ˆì •ì ì¸ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
            "\n"
            "[ê·œì¹™]\n"
            "1. ì‘ë‹µì€ ë°˜ë“œì‹œ ì œê³µí•œ ëª¨ë“  ETF ëª©ë¡ì„ ë¶„ì„í•œ í›„ì—, ìœ„ì˜ [ì¶œë ¥ í¬ë§·]ì„ ì •í™•í•˜ê²Œ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n"
            "2. ê° ETFëŠ” `#### <ì‹¬ë³¼> (<ì´ë¦„>)` í˜•ì‹ì˜ ì œëª©ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "3. ê° ETF ì •ë³´ëŠ” `- **ê¶Œê³  ì‚¬í•­**: ...`ê³¼ `- **ì´ìœ **: ...` í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "4. `### ì¢…í•© ì˜ê²¬:` í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "5. í¬ë§· ì™¸ì— ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§, ì„œë¡ , ê²°ë¡  ë“± ë¶€ì—° ì„¤ëª…ì„ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤."
        )

        # 4. ì˜¤ëŠ˜ ë‚ ì§œ (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
        kst_now = get_kst_now()
        today_date = f"[ë¶„ì„ ê¸°ì¤€ì¼] {kst_now.year}ë…„ {kst_now.month}ì›” {kst_now.day}ì¼"
        
        # 5. ìµœì¢… developer ë©”ì‹œì§€ ì¡°ë¦½
        developer_content = (
            f"ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì •ì— ëŒ€í•œ ì¡°ì–¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ [ê·œì¹™]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì‹­ì‹œì˜¤.\n\n"
            f"{user_info}\n\n"
            f"{etf_info}\n\n"
            f"{today_date}\n\n"
            f"{output_format_and_rules}"
        )
        
        # 6. user ë©”ì‹œì§€(ëª…ë ¹) ë‹¨ìˆœí™”
        user_content = "ì˜¤ëŠ˜ì˜ íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì • ì¡°ì–¸ì„ ìƒì„±í•´ì¤˜."

        messages = [
            {"role": "system", "content": developer_content}, # ì—­í• ì„ systemìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë” ê°•ë ¥í•œ ì§€ì‹œ
            {"role": "user", "content": user_content}
        ]
        return messages
    except Exception as e:
        logger.error(f"âŒ í†µí•© ë¶„ì„ ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return []

async def request_ai_analysis(
    messages: list, 
    api_key: str, 
    model_type: str
) -> Optional[str]:
    """ETF_AI ì„œë¹„ìŠ¤ì— ë¶„ì„ ìš”ì²­ - analyze_sentiment í•¨ìˆ˜ ì‚¬ìš© (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    
    import asyncio
    
    for attempt in range(MAX_RETRIES):
        try:
            logger.info(f"ğŸ”„ AI ì„œë¹„ìŠ¤ ìš”ì²­ ì‹œë„ {attempt + 1}/{MAX_RETRIES}")
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{AI_SERVICE_URL}/analyze",
                    json={
                        "messages": messages,
                        "api_key": api_key,
                        "model_type": model_type
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("success", False):
                        processing_time = result.get("processing_time", 0)
                        logger.info(f"âœ… AI ë¶„ì„ ì„±ê³µ (ì‹œë„ {attempt + 1}, ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
                        return result.get("answer", "")
                    else:
                        error_msg = result.get('error', 'Unknown error')
                        logger.error(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {error_msg}")
                        return None
                else:
                    logger.error(f"âŒ AI ì„œë¹„ìŠ¤ HTTP ì˜¤ë¥˜: {response.status_code}")
                    if attempt < MAX_RETRIES - 1:
                        await asyncio.sleep(RETRY_DELAY)
                        continue
                    return None
                    
        except httpx.TimeoutException:
            logger.warning(f"â° AI ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1})")
            if attempt < MAX_RETRIES - 1:
                await asyncio.sleep(RETRY_DELAY)
                continue
            return None
            
        except httpx.ConnectError:
            logger.error(f"ğŸ”Œ AI ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {AI_SERVICE_URL}")
            if attempt < MAX_RETRIES - 1:
                await asyncio.sleep(RETRY_DELAY)
                continue
            return None
            
        except Exception as e:
            logger.error(f"âŒ AI ì„œë¹„ìŠ¤ ìš”ì²­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
            if attempt < MAX_RETRIES - 1:
                await asyncio.sleep(RETRY_DELAY)
                continue
            return None
    
    logger.error(f"âŒ AI ì„œë¹„ìŠ¤ ìš”ì²­ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ({MAX_RETRIES}íšŒ)")
    return None

async def request_batch_ai_analysis(
    analysis_requests: list
) -> list:
    """ETF_AI ì„œë¹„ìŠ¤ì— ë°°ì¹˜ ë¶„ì„ ìš”ì²­ - ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›"""
    
    import asyncio
    
    try:
        logger.info(f"ğŸ”„ ë°°ì¹˜ AI ë¶„ì„ ìš”ì²­ ì‹œì‘: {len(analysis_requests)}ê°œ")
        
        async with httpx.AsyncClient(timeout=120.0) as client:  # ë°°ì¹˜ ì²˜ë¦¬ì´ë¯€ë¡œ ë” ê¸´ íƒ€ì„ì•„ì›ƒ
            response = await client.post(
                f"{AI_SERVICE_URL}/analyze/batch",
                json={
                    "requests": [
                        {
                            "messages": req["messages"],
                            "api_key": req["api_key"],
                            "model_type": req["model_type"]
                        }
                        for req in analysis_requests
                    ]
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get("success", False):
                    summary = result.get("summary", {})
                    logger.info(f"âœ… ë°°ì¹˜ AI ë¶„ì„ ì„±ê³µ: {summary.get('successful_count', 0)}ê°œ ì„±ê³µ, {summary.get('failed_count', 0)}ê°œ ì‹¤íŒ¨, ì´ ì‹œê°„: {summary.get('total_processing_time', 0):.2f}ì´ˆ")
                    
                    # ì„±ê³µí•œ ê²°ê³¼ë“¤ë§Œ ë°˜í™˜
                    successful_results = result.get("results", {}).get("successful", [])
                    return [res.get("answer", "") for res in successful_results]
                else:
                    error_msg = result.get('error', 'Unknown error')
                    logger.error(f"âŒ ë°°ì¹˜ AI ë¶„ì„ ì‹¤íŒ¨: {error_msg}")
                    return []
            else:
                logger.error(f"âŒ ë°°ì¹˜ AI ì„œë¹„ìŠ¤ HTTP ì˜¤ë¥˜: {response.status_code}")
                return []
                
    except httpx.TimeoutException:
        logger.warning(f"â° ë°°ì¹˜ AI ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ")
        return []
        
    except httpx.ConnectError:
        logger.error(f"ğŸ”Œ ë°°ì¹˜ AI ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {AI_SERVICE_URL}")
        return []
        
    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ AI ì„œë¹„ìŠ¤ ìš”ì²­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return []

def parse_structured_ai_response(analysis_text: str) -> dict:
    """
    êµ¬ì¡°í™”ëœ AI ë¶„ì„ ì‘ë‹µ í…ìŠ¤íŠ¸(ë§ˆí¬ë‹¤ìš´ í˜•ì‹)ë¥¼ íŒŒì‹±í•˜ì—¬ ë”•ì…”ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    import re
    parsed_data = {"etfs": [], "summary": ""}
    try:
        # '### ì¢…í•© ì˜ê²¬:'ì„ ê¸°ì¤€ìœ¼ë¡œ ì¢…í•© ì˜ê²¬ ì¶”ì¶œ
        summary_match = re.search(r'### ì¢…í•© ì˜ê²¬:\s*(.*)', analysis_text, re.DOTALL | re.IGNORECASE)
        if summary_match:
            parsed_data["summary"] = summary_match.group(1).strip()
            etf_section = analysis_text[:summary_match.start()]
        else:
            etf_section = analysis_text

        # '####'ë¡œ ì‹œì‘í•˜ëŠ” ê° ETF ë¸”ë¡ì„ ì°¾ì•„ì„œ ì²˜ë¦¬
        etf_blocks = re.split(r'(?=####\s+)', etf_section)

        for block in etf_blocks:
            block = block.strip()
            if not block.startswith('####'):
                continue
            
            # ì‹¬ë³¼ê³¼ ì´ë¦„ ì¶”ì¶œ
            title_match = re.search(r'####\s+([A-Z0-9]+)\s*\((.*?)\)', block, re.IGNORECASE)
            if not title_match:
                continue
            
            symbol, name = title_match.groups()

            # ê¶Œê³  ì‚¬í•­ ì¶”ì¶œ
            recommendation_match = re.search(r'-\s*\*\*ê¶Œê³  ì‚¬í•­\*\*:\s*(.*)', block, re.IGNORECASE)
            recommendation = recommendation_match.group(1).strip() if recommendation_match else ""

            # ì´ìœ  ì¶”ì¶œ
            reason_match = re.search(r'-\s*\*\*ì´ìœ \*\*:\s*(.*)', block, re.IGNORECASE | re.DOTALL)
            reason = reason_match.group(1).strip() if reason_match else ""

            parsed_data["etfs"].append({
                "symbol": symbol.strip(),
                "name": name.strip(),
                "recommendation": recommendation,
                "reason": reason
            })

    except Exception as e:
        logger.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ summaryì— ë„£ì–´ ê¸°ì¡´ ë¡œì§ì´ ì–´ëŠì •ë„ ë™ì‘í•˜ë„ë¡ í•¨
        return {"etfs": [], "summary": analysis_text}
    
    logger.debug(f"íŒŒì‹±ëœ ë°ì´í„°: {parsed_data}")
    return parsed_data

def determine_notification_need(
    db,
    user: User,
    analysis_result: str
) -> tuple[bool, dict]:
    """
    ì´ì „ ë¶„ì„ ê²°ê³¼ì™€ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•Œë¦¼ í•„ìš”ì„± íŒë‹¨
    - ì˜¤ëŠ˜ì˜ ì²« ë¶„ì„ì€ í•­ìƒ ì•Œë¦¼ ì „ì†¡
    """
    if not embedding_model:
        logger.error(" embedding ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì•Œë¦¼ íŒë‹¨ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return True, {"etfs": [], "summary": analysis_result}

    try:
        logger.debug(f"ğŸš€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì•Œë¦¼ í•„ìš”ì„± íŒë‹¨ ì‹œì‘ (ì‚¬ìš©ì: {user.id})")
        
        # 1. í˜„ì¬ ë¶„ì„ ê²°ê³¼ íŒŒì‹±
        parsed_analysis = parse_structured_ai_response(analysis_result)
        
        # 2. ì´ì „ ë¶„ì„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        previous_analysis = user.settings.last_analysis_result
        last_analysis_time = user.settings.last_analysis_at

        # 3. ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•  ì¤€ë¹„
        current_time = datetime.now(last_analysis_time.tzinfo if last_analysis_time else None)
        new_setting_data = {
            "last_analysis_result": analysis_result,
            "last_analysis_at": current_time
        }

        # 4. ì˜¤ëŠ˜ì˜ ì²« ë¶„ì„ì¸ì§€ í™•ì¸
        is_first_analysis_today = not last_analysis_time or last_analysis_time.date() < current_time.date()

        if is_first_analysis_today:
            logger.info(f"âœ… ì˜¤ëŠ˜ì˜ ì²« ë¶„ì„ì…ë‹ˆë‹¤. ì•Œë¦¼ì„ ì „ì†¡í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
            update_user_investment_settings(db, user.id, new_setting_data)
            return True, parsed_analysis

        # 5. ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° (ì˜¤ëŠ˜ ì²« ë¶„ì„ì´ ì•„ë‹Œë° ì´ì „ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°)
        if not previous_analysis:
            logger.warning(f"âš ï¸ ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì•Œë¦¼ì„ ì „ì†¡í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
            update_user_investment_settings(db, user.id, new_setting_data)
            return True, parsed_analysis

        # 6. ì´ì „ê³¼ í˜„ì¬ ë¶„ì„ì˜ "ì¢…í•© ì˜ê²¬"ì„ ì¶”ì¶œí•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚°
        current_summary = parsed_analysis.get("summary", "")
        previous_parsed = parse_structured_ai_response(previous_analysis)
        previous_summary = previous_parsed.get("summary", "")

        # ì¢…í•© ì˜ê²¬ì´ ì—†ëŠ” ê²½ìš°, ë¹„êµê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ë³€í™”ë¡œ ê°„ì£¼
        if not current_summary or not previous_summary:
            logger.warning("í˜„ì¬ ë˜ëŠ” ì´ì „ ë¶„ì„ì—ì„œ 'ì¢…í•© ì˜ê²¬'ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´, ì¤‘ìš”í•œ ë³€ê²½ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤.")
            update_user_investment_settings(db, user.id, new_setting_data)
            return True, parsed_analysis

        logger.debug("--- í˜„ì¬ ì¢…í•© ì˜ê²¬ ---")
        logger.debug(current_summary)
        logger.debug("--- ì´ì „ ì¢…í•© ì˜ê²¬ ---")
        logger.debug(previous_summary)
        logger.debug("--------------------")
        
        embedding_current = embedding_model.encode([current_summary])
        embedding_previous = embedding_model.encode([previous_summary])
        
        similarity = cosine_similarity(embedding_current, embedding_previous)[0][0]
        
        logger.debug(f"ğŸ“Š ì´ì „ ê²°ê³¼ì™€ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {similarity:.4f}")

        # 7. ìœ ì‚¬ë„ ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì•Œë¦¼ ì—¬ë¶€ ê²°ì •
        SIMILARITY_THRESHOLD = 0.95
        
        should_notify = False
        if similarity < SIMILARITY_THRESHOLD:
            logger.info(f"âœ… ìœ ì‚¬ë„({similarity:.4f})ê°€ ì„ê³„ê°’({SIMILARITY_THRESHOLD}) ë¯¸ë§Œ. ì¤‘ìš”í•œ ë³€í™”ë¡œ íŒë‹¨í•˜ì—¬ ì•Œë¦¼ì„ ì „ì†¡í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
            should_notify = True
            # ì•Œë¦¼ì„ ë³´ë‚¼ ë•Œë§Œ ìµœì‹  ë¶„ì„ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
            update_user_investment_settings(db, user.id, new_setting_data)
        else:
            logger.info(f"âŒ ìœ ì‚¬ë„({similarity:.4f})ê°€ ì„ê³„ê°’({SIMILARITY_THRESHOLD}) ì´ìƒ. ë³€í™”ê°€ ë¯¸ë¯¸í•˜ì—¬ ì•Œë¦¼ì„ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            # ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•Šìœ¼ë¯€ë¡œ ê²°ê³¼ë„ ì €ì¥í•˜ì§€ ì•ŠìŒ

        return should_notify, parsed_analysis
        
    except Exception as e:
        logger.error(f"âŒ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì•Œë¦¼ íŒë‹¨ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ëŠ” ì¼ë‹¨ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” ê²ƒì„ ê¸°ë³¸ìœ¼ë¡œ í•¨
        return True, {"etfs": [], "summary": analysis_result}
