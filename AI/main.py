from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from model.model import create_response, analyze_sentiment
import uvicorn
import json
import asyncio
import time
from typing import List, Dict, Any
from tunning.instructions import instructions
from concurrent.futures import ThreadPoolExecutor
import logging
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="ETF AI Analysis Service", version="1.0.0")

# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ë ˆë“œ í’€
executor = ThreadPoolExecutor(max_workers=10)

class ChatRequest(BaseModel):
    messages: List[dict]  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬
    api_key: str
    model_type: str

class PersonaRequest(BaseModel):
    name: str
    invest_type: int
    interest: List[str]

class BatchAnalyzeRequest(BaseModel):
    requests: List[ChatRequest]  # ì—¬ëŸ¬ ë¶„ì„ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬

@app.post("/chat/stream")
async def chat_stream_endpoint(req: ChatRequest):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸"""
    
    async def generate_stream():
        try:
            # ë°±ì—”ë“œì—ì„œ ì „ì†¡í•œ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš©
            stream, updated_messages = create_response(req.messages, req.api_key, req.model_type)
            
            for chunk in stream:
                delta = getattr(chunk.choices[0], "delta", None)
                if delta and hasattr(delta, "content") and delta.content:
                    yield f"data: {json.dumps({'content': delta.content})}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_message = f"AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {str(e)}"
            yield f"data: {json.dumps({'content': error_message})}\n\n"
            yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

@app.post("/persona")
async def get_persona(req: PersonaRequest):
    persona = instructions(req.name, req.invest_type, req.interest)
    return {"persona": persona}

@app.post("/analyze")
async def analyze_endpoint(req: ChatRequest):
    """íˆ¬ì ë¶„ì„ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸ - analyze_sentiment í•¨ìˆ˜ ì‚¬ìš© (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""
    start_time = time.time()
    
    try:
        # ìŠ¤ë ˆë“œ í’€ì—ì„œ ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        analysis_result, updated_messages = await loop.run_in_executor(
            executor, 
            analyze_sentiment, 
            req.messages, 
            req.api_key, 
            req.model_type
        )
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… AI ë¶„ì„ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
        logger.info(f"âœ… AI ë¶„ì„ ê²°ê³¼: {analysis_result}")
        
        return {
            "answer": analysis_result,
            "success": True,
            "processing_time": processing_time
        }
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨ ({processing_time:.2f}ì´ˆ): {e}")
        
        return {
            "answer": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "success": False,
            "error": str(e),
            "processing_time": processing_time
        }

@app.post("/analyze/batch")
async def batch_analyze_endpoint(req: BatchAnalyzeRequest):
    """ì—¬ëŸ¬ íˆ¬ì ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    start_time = time.time()
    logger.info(f"ğŸ”„ ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(req.requests)}ê°œ ìš”ì²­")
    
    try:
        # ëª¨ë“  ë¶„ì„ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        async def analyze_single(request: ChatRequest) -> Dict[str, Any]:
            single_start_time = time.time()
            
            try:
                loop = asyncio.get_event_loop()
                analysis_result, updated_messages = await loop.run_in_executor(
                    executor,
                    analyze_sentiment,
                    request.messages,
                    request.api_key,
                    request.model_type
                )
                
                single_processing_time = time.time() - single_start_time
                logger.info(f"âœ… ë‹¨ì¼ ë¶„ì„ ì™„ë£Œ ({single_processing_time:.2f}ì´ˆ)")
                logger.info(f"âœ… AI ë¶„ì„ ê²°ê³¼: {analysis_result}")
                
                return {
                    "success": True,
                    "answer": analysis_result,
                    "processing_time": single_processing_time,
                    "request_id": id(request)  # ìš”ì²­ ì‹ë³„ìš©
                }
                
            except Exception as e:
                single_processing_time = time.time() - single_start_time
                logger.error(f"âŒ ë‹¨ì¼ ë¶„ì„ ì‹¤íŒ¨ ({single_processing_time:.2f}ì´ˆ): {e}")
                
                return {
                    "success": False,
                    "error": str(e),
                    "processing_time": single_processing_time,
                    "request_id": id(request)
                }
        
        # ëª¨ë“  ìš”ì²­ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        tasks = [analyze_single(request) for request in req.requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        successful_results = []
        failed_results = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_results.append({
                    "index": i,
                    "error": str(result),
                    "request_id": id(req.requests[i])
                })
            elif result.get("success"):
                successful_results.append(result)
            else:
                failed_results.append(result)
        
        total_processing_time = time.time() - start_time
        
        logger.info(f"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: ì„±ê³µ {len(successful_results)}ê°œ, ì‹¤íŒ¨ {len(failed_results)}ê°œ ({total_processing_time:.2f}ì´ˆ)")
        
        return {
            "success": True,
            "results": {
                "successful": successful_results,
                "failed": failed_results
            },
            "summary": {
                "total_requests": len(req.requests),
                "successful_count": len(successful_results),
                "failed_count": len(failed_results),
                "success_rate": len(successful_results) / len(req.requests) if req.requests else 0,
                "total_processing_time": total_processing_time,
                "avg_processing_time": total_processing_time / len(req.requests) if req.requests else 0
            }
        }
        
    except Exception as e:
        total_processing_time = time.time() - start_time
        logger.error(f"âŒ ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨ ({total_processing_time:.2f}ì´ˆ): {e}")
        
        return {
            "success": False,
            "error": str(e),
            "processing_time": total_processing_time
        }

@app.get("/")
async def root():
    """Railway í—¬ìŠ¤ì²´í¬ìš© ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "ETF AI Analysis Service is running",
        "status": "healthy",
        "service": "ETF AI Analysis Service",
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    """AI ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "ETF AI Analysis Service",
        "timestamp": time.time(),
        "thread_pool_size": executor._max_workers
    }

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8001))  # Railway ê¸°ë³¸ í¬íŠ¸ëŠ” 8001
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=port, 
        reload=False,
        log_level="info"
    ) 